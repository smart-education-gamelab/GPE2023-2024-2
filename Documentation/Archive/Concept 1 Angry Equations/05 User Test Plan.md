# Angry Equations: User Test Plan

## Test Context

> How did we get these metrics from our subjects (hboi)?
> How did we reach these metrics and/or benchmark?
> Why are we testing this?
> Benchmark document?
> learning goals, design pillars etc. -> Why are these metrics important?

<!--
### Personas

The predefined persona most fitting to do the test:

1. Maya Mathlete
2. Avery Average Learner
3. Daisy Dilemma

### User Journey

Describe the user journeys of our personas for concept 1.
-->

## Metrics

Based on our user journeys we have decided upon the following metrics:

### During the Game

1. Start off the game

   - How many clicks does the player require to open/start the level?
2. Start of the level

   - How many seconds does it take the player to figure out the control scheme?
   - Are the players able to complete each level within a set amount of time?
3. End the level

   - Are the players able to finish each level within a set amount of time?
4. End of game

   - Did the player ask to continue the game?

### Observations

- How many times does the player ask the test supervisor how the UI works?
- Did the player make a remark about the difficulty of the game (i.e. too easy/difficult)?
- Can the player hit an enemy within their first 2 tries?
- How many times would the players let themselves be interrupted / distracted?
- How many times did the players ask questions?
- Did the player ask if they could continue to play?

### Interview Afterwards

#### Fun

- Would you continue to play?
- _If no: Would you continue playing if we made more content?_
- Why?

#### Satisfaction

- How do you feel after completing the game?
- Do you feel satisfied?
- Why?

#### Miscellaneous

- Why did you (not) feel time pressure to finish the game?
- Why do you think the gameplay is (not) challenging?
- Do you know what mathematical concepts were in the game?
- Would you play this game to improve your math abilities?
- Did you feel like the game was doable with your skill set (math knowledge)?

## Benchmarks

We have the following benchmarks based on assumptions of the metrics:

### During the Game

1. Start off the game

   - How many clicks does the player require to open/start the level?

     - [ ] 0 times
     - [ ] 1 to 5 times
     - [ ] 5 or more
2. Start of the level

   - Are the players able to complete each level within a set amount of time?

     - level 1: ~~1 minute 30 seconds~~
     - level 2: ~~2 minutes~~
     - (demo) level 3: ~~2 minutes 30 seconds~~

### Observations

- How many times does the player ask the test supervisor how the UI works?

  - [ ] 0 times
  - [ ] 1 to 3 times
  - [ ] 3 or more
- Did the player make a remark about the difficulty of the game (i.e. too easy/difficult)?

  - [ ] Yes
  - [ ] No
- Can the player hit an enemy within their first 2 tries?

  - [ ] Yes
  - [ ] No
- How many times would the players let themselves be interrupted / distracted?

  - [ ] Yes
  - [ ] No
- How many times did the players ask questions?

  - [ ] 0 times
  - [ ] 1 to 3 times
  - [ ] 3 or more
- Did the player ask if they could continue to play?

  - [ ] Yes
  - [ ] No

### Interview Afterwards

#### Fun

- Would you continue to play?

  - [ ] Yes
  - [ ] No

#### Satisfaction

- Do you feel satisfied?

  - [ ] Yes
  - [ ] No
